{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10027310,"sourceType":"datasetVersion","datasetId":6175078},{"sourceId":10128522,"sourceType":"datasetVersion","datasetId":6155701},{"sourceId":10198939,"sourceType":"datasetVersion","datasetId":6250725},{"sourceId":10209719,"sourceType":"datasetVersion","datasetId":6310057},{"sourceId":10384012,"sourceType":"datasetVersion","datasetId":6413046},{"sourceId":10442999,"sourceType":"datasetVersion","datasetId":6463796}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Data analysis\nfrom typing import Dict\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\n# Modelling\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.models import Sequential\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.utils.class_weight import compute_class_weight\nimport transformers\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\nfrom transformers import (ElectraForSequenceClassification,\n                          ElectraTokenizerFast, EvalPrediction, InputFeatures,\n                          Trainer, TrainingArguments, glue_compute_metrics, pipeline,\n                            AutoTokenizer, AutoModelForSequenceClassification)\n\n\n# Supressing warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Metrics\nfrom sklearn.metrics import f1_score, precision_score, recall_score, matthews_corrcoef, roc_auc_score, accuracy_score\nfrom transformers.data.metrics import glue_compute_metrics\nfrom scipy.stats import spearmanr\n\n\n# Other imports\nimport os\nimport random\nimport gc\nimport pickle\n\n\n# Logging into weights and biases\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"WB_token\")\nwandb.login(key=secret_value_0)\n\n\n# Methods for setting random seed\ndef set_seeds(seed=123):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    \n\n# Printing version of the transformer library\ntransformers.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:25:56.448922Z","iopub.execute_input":"2025-01-11T13:25:56.449548Z","iopub.status.idle":"2025-01-11T13:26:20.306557Z","shell.execute_reply.started":"2025-01-11T13:25:56.449512Z","shell.execute_reply":"2025-01-11T13:26:20.305664Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.23k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e3b161060db4b858509471225667370"}},"metadata":{}},{"name":"stderr","text":"The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"137501d1b4054176a13732fc4af9658b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d45688c782c496896f7f1c1f3330d3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf7599c6e85b446cb73be1ec7f361b35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fad634bec8d048b49e28410f303777d0"}},"metadata":{}},{"name":"stderr","text":"The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\nYou are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nThe `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'4.45.1'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Global variables\ntask = \"eraser_movie\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading DistilBERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=2)\n\n# Setting up device for DistilBERT training\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice_cpu = torch.device('cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:26:20.307944Z","iopub.execute_input":"2025-01-11T13:26:20.308497Z","iopub.status.idle":"2025-01-11T13:26:21.566858Z","shell.execute_reply.started":"2025-01-11T13:26:20.308470Z","shell.execute_reply":"2025-01-11T13:26:21.566083Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58fe18b8d67e4a45bfcf2df41b5dbfb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09187e4110d44c7a982b22599e298c98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d3553b777944f6c8fe1720d905c1df4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f21c6a47000f4b7bbf81bd55cb27ada6"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# class used to store datasets in this project (required for object loading from pickle)\nclass TrainerDataset(Dataset):\n    def __init__(self, inputs, targets, tokenizer, evidences=None):\n        self.inputs = inputs\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.evidences=evidences\n\n        # Tokenize the input\n        self.tokenized_inputs = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")   \n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        return InputFeatures(\n            input_ids=self.tokenized_inputs['input_ids'][idx],\n#             token_type_ids=self.tokenized_inputs['token_type_ids'][idx],\n            attention_mask=self.tokenized_inputs['attention_mask'][idx],\n            label=self.targets[idx])   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:26:21.602392Z","iopub.execute_input":"2025-01-11T13:26:21.603006Z","iopub.status.idle":"2025-01-11T13:26:21.610700Z","shell.execute_reply.started":"2025-01-11T13:26:21.602975Z","shell.execute_reply":"2025-01-11T13:26:21.609940Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Set seed for reproducibility\nnp.random.seed(123)\n\n\n# Method used to calculate all discriminatory power metrics (AUC, f1, Accuracy, ...) and spearman correlation\ndef calculate_metric_values(predictions, label_ids) -> Dict:\n\n    # Getting predicted target variable from probas\n    preds = np.argmax(predictions, axis=1)\n\n    # Compute GLUE task metrics\n    task_metrics = glue_compute_metrics(\n        task_name=\"sst-2\",\n        preds=preds,\n        labels=label_ids\n    )\n    \n    # Compute additional metrics\n    accuracy = accuracy_score(label_ids, preds)\n    f1 = f1_score(label_ids, preds, average=\"binary\")  # Binary classification\n    precision = precision_score(label_ids, preds, average=\"binary\")\n    recall = recall_score(label_ids, preds, average=\"binary\")\n    mcc = matthews_corrcoef(label_ids, preds)\n    \n    # Compute AUC\n    # For AUC, we need the raw predictions (probabilities). Assuming predictions are logits:\n    if len(predictions.shape) > 1:  # Check if predictions are logits\n        probs = np.exp(predictions) / np.exp(predictions).sum(axis=1, keepdims=True)\n        auc = roc_auc_score(label_ids, probs[:, 1])  # Assuming binary classification\n    else:\n        auc = None  # AUC cannot be calculated without probabilities\n\n\n    # Check calibration\n    df = pd.DataFrame(data={\"pred\":np.transpose(predictions)[1], \"real\":label_ids })\n    df[\"centile\"] = 0\n    for i in range(1,10):\n        less_range = np.quantile(df.pred,i/10)\n        df[\"centile\"] = [df.centile[j]+1 if df[\"pred\"][j]>less_range else df.centile[j] for j in range(len(df.centile))]\n    stats_for_bucket = df.groupby('centile').mean()\n    correlation, p_value = spearmanr(stats_for_bucket[\"pred\"], stats_for_bucket[\"real\"])\n    \n    # Add custom metrics to the output dictionary\n    metrics =  {\n        \"acc\": accuracy,\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"mcc\": mcc,\n        \"auc\": auc,\n        \"spearman\": correlation,\n        \"spearman_pval\": p_value\n    }\n    task_metrics.update(metrics)\n\n    return task_metrics\n\n# Wrapper of the above method for DistilBERT\ndef compute_metrics(p: EvalPrediction) -> Dict:\n    return calculate_metric_values(p.predictions, p.label_ids)\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:26:21.611750Z","iopub.execute_input":"2025-01-11T13:26:21.612049Z","iopub.status.idle":"2025-01-11T13:26:21.622448Z","shell.execute_reply.started":"2025-01-11T13:26:21.612024Z","shell.execute_reply":"2025-01-11T13:26:21.621760Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# DistilBERT","metadata":{}},{"cell_type":"code","source":"os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\n# Method used to train model using given train dataset and evaluates it on eval_dataset\ndef train_model(train_dataset, eval_dataset, output_dir, model_name = \"DistilBERT\"):\n    set_seeds(123)\n\n    if model_name == \"DistilBERT\":\n        # Setting up training arguments\n        tokenizer = None\n        training_args = TrainingArguments(\n            output_dir= output_dir,\n            num_train_epochs=2, \n            overwrite_output_dir=True,\n            do_train=True,\n            do_eval=True,\n            per_device_train_batch_size=16,    \n            dataloader_drop_last=True,  # Make sure all batches are of equal size\n        )\n\n        # Initializing the model\n        model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=2, ignore_mismatched_sizes=True)\n        model.to(device)\n\n        # Setting up trainer with weighted loss\n        k, v = np.unique(train_dataset.targets, return_counts=True)\n        trainer = CustomTrainer(\n            model=model,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            compute_metrics=compute_metrics,\n            class_weights = list((v/np.sum(v)).astype(np.float32)))\n\n        # Model training and evaluation\n        trainer.train()\n        model_result = trainer.evaluate()\n        print(f\"{model_name} accuracy: {model_result['eval_acc']}\")\n        \n\n    \n    elif model_name == \"LSTM\":\n        # Hyperparameters \n        vocab_size = 10000\n        max_sequence_length = 300\n        embedding_dim = 100\n        lstm_units = 128\n        dropout_rate = 0.5\n        num_classes = 2\n\n        # Tokenization\n        tokenizer = Tokenizer(num_words=vocab_size)\n        tokenizer.fit_on_texts(train_dataset.inputs)\n        sequences = tokenizer.texts_to_sequences(train_dataset.inputs)\n        sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n\n        # Model definition\n        model = Sequential()\n        model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n        model.add(LSTM(units=lstm_units, return_sequences=False))\n        model.add(Dropout(rate=dropout_rate))\n        model.add(Dense(units=1, activation='sigmoid'))\n\n        # class_weights = compute_class_weight(\n        #     class_weight='balanced',\n        #     classes=np.unique(np.array(train_dataset.targets)),\n        #     y=np.array(train_dataset.targets)\n        # )\n        # class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n        \n        # Compile the model\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n        # Train the model\n        model.fit(sequences, np.array(\n            train_dataset.targets),\n                  batch_size=32, \n                  epochs=5, \n                  # class_weight=class_weights\n                 )\n\n        # Evaluate the model\n        tokenized_eval_dataset = pad_sequences(tokenizer.texts_to_sequences(eval_dataset.inputs))\n        pred = model.predict(tokenized_eval_dataset)\n        pred = np.array([[1-x[0], x[0]] for x in pred])\n        model_result = calculate_metric_values(pred, eval_dataset.targets)\n        print(f\"{model_name} accuracy: {model_result['acc']}\")\n\n    elif model_name==\"SVM\":\n        # Tokenization\n        tokenizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n        X = tokenizer.fit_transform(train_dataset.inputs)\n\n        # Model training\n        model = SVC(kernel='rbf', C=1.0, probability=True, class_weight = \"balanced\")\n        model.fit(X, train_dataset.targets)\n\n        # Evaluating the results\n        pred = model.predict_proba(tokenizer.transform(eval_dataset.inputs))\n        model_result = calculate_metric_values(pred, eval_dataset.targets)\n        print(f\"{model_name} accuracy: {model_result['acc']}\")\n    \n    return model, model_result, tokenizer\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:26:21.623471Z","iopub.execute_input":"2025-01-11T13:26:21.623718Z","iopub.status.idle":"2025-01-11T13:26:21.692028Z","shell.execute_reply.started":"2025-01-11T13:26:21.623695Z","shell.execute_reply":"2025-01-11T13:26:21.691121Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class CustomTrainer(Trainer):\n    \n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.class_weights = class_weights\n        \n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.get(\"labels\")\n        # forward pass\n        outputs = model(**inputs)\n        logits = outputs.get('logits')\n        \n        # move weights to correct device\n        device = logits.device\n        weight = torch.tensor(self.class_weights, device=device)\n        \n        # compute custom loss\n        loss_fct = nn.CrossEntropyLoss(weight=weight)\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n\n\n\n\nmodels = {}\nset_seeds(123)\n\n# Loading original train and original eval datasets before imbalancing\nwith open(f\"/kaggle/input/balanced-datasets/{task}_train.obj\", 'rb') as pickle_file:\n    train_dataset = pickle.load(pickle_file)\nwith open(f\"/kaggle/input/balanced-datasets/{task}_eval.obj\", 'rb') as pickle_file:\n    eval_dataset = pickle.load(pickle_file)\n    \n\n# Training baseline models on original data before imbalancing\nfor model_name in [\"DistilBERT\"]:\n# for model_name in [\"LSTM\", \"SVM\"]:\n    # Training model\n    model, models[f\"{task}_100_original_0*{model_name}\"], tok = train_model(train_dataset, eval_dataset, f\"models/{task}/\", model_name)\n    # Saving model\n    filehandler = open(f\"{task}_100_original_0*{model_name}\"+\".obj\",\"wb\")\n    pickle.dump(model,filehandler)\n    filehandler.close()\n\n    if model_name != \"DistilBERT\":\n        # Saving tokenizer\n        filehandler = open(\"tok_\"+f\"{task}_100_original_0*{model_name}\"+\".obj\",\"wb\")\n        pickle.dump(tok,filehandler)\n        filehandler.close()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:26:23.999233Z","iopub.execute_input":"2025-01-11T13:26:23.999866Z","iopub.status.idle":"2025-01-11T13:26:24.058756Z","shell.execute_reply.started":"2025-01-11T13:26:23.999836Z","shell.execute_reply":"2025-01-11T13:26:24.058065Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"skip = 0   # Some codes had to be run multiple times this value tells how many datasets should be skipped before model training\nmodel_count = len(os.listdir(f\"/kaggle/input/paraphrase-dataset-final/\"))-skip\n\nfor model_name in [\"DistilBERT\"]:\n# for model_name in [\"LSTM\", \"SVM\"]:\n    for task in [\"eraser_movie\"]:\n        for file in os.listdir(f\"/kaggle/input/paraphrase-dataset-final/\")[skip:skip+model_count]:\n            # Printing the name of dataset the model is being trained on\n            print(file)\n\n            # Loading training data, training the model, and evaluating the results\n            with open(\"/kaggle/input/paraphrase-dataset-final/\"+file, 'rb') as pickle_file:\n                model, pred, tok = train_model(pickle.load(pickle_file), eval_dataset, f\"models/{task}/\", model_name)\n\n                # Correcting some file names\n                if \"paraphrase\" in file:\n                    parts = file.split(\"paraphrase\")\n                    file = parts[0]+\"paraphrase\"+parts[1][3:]\n                    \n                # Saving model\n                filehandler = open(file.split(\".obj\")[0]+ \"*\"+model_name+\".obj\",\"wb\")\n                pickle.dump(model,filehandler)\n                filehandler.close()\n\n                if model_name != \"DistilBERT\":\n                    # Saving tokenizer\n                    filehandler = open(\"tok_\"+file.split(\".obj\")[0]+ \"*\"+model_name+\".obj\",\"wb\")\n                    pickle.dump(tok,filehandler)\n                    filehandler.close()\n                \n                models[file.split(\".obj\")[0]+ \"*\"+model_name] = pred\n                gc.collect()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:26:24.060037Z","iopub.execute_input":"2025-01-11T13:26:24.060862Z","iopub.status.idle":"2025-01-11T13:39:58.015485Z","shell.execute_reply.started":"2025-01-11T13:26:24.060833Z","shell.execute_reply":"2025-01-11T13:39:58.014598Z"}},"outputs":[{"name":"stdout","text":"eraser_movie_20_paraphrase_10_0.obj\nEpoch 1/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - accuracy: 0.6179 - loss: 0.6894\nEpoch 2/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.7771 - loss: 0.5161\nEpoch 3/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9087 - loss: 0.2199\nEpoch 4/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.8284 - loss: 0.4595\nEpoch 5/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9208 - loss: 0.2464\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step\nLSTM accuracy: 0.55\neraser_movie_10_paraphrase_10_1.obj\nEpoch 1/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6452 - loss: 0.6691\nEpoch 2/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9473 - loss: 0.2186\nEpoch 3/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9806 - loss: 0.0776\nEpoch 4/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9792 - loss: 0.1045\nEpoch 5/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9812 - loss: 0.0665\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step\nLSTM accuracy: 0.5\neraser_movie_50_paraphrase_10_3.obj\nEpoch 1/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5305 - loss: 0.6936\nEpoch 2/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.6340 - loss: 0.7034\nEpoch 3/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.7939 - loss: 0.5572\nEpoch 4/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8851 - loss: 0.3498\nEpoch 5/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9351 - loss: 0.1863\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step\nLSTM accuracy: 0.645\neraser_movie_50_paraphrase_10_2.obj\nEpoch 1/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5172 - loss: 0.6908\nEpoch 2/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.6624 - loss: 0.7613\nEpoch 3/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8459 - loss: 0.4501\nEpoch 4/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9459 - loss: 0.1530\nEpoch 5/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8614 - loss: 0.2997\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step\nLSTM accuracy: 0.555\neraser_movie_10_paraphrase_10_4.obj\nEpoch 1/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5948 - loss: 0.6734\nEpoch 2/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9095 - loss: 0.2888\nEpoch 3/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9682 - loss: 0.0836\nEpoch 4/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9759 - loss: 0.0587\nEpoch 5/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9811 - loss: 0.0495\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step\nLSTM accuracy: 0.5\neraser_movie_50_paraphrase_10_1.obj\nEpoch 1/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.5353 - loss: 0.6936\nEpoch 2/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.7226 - loss: 0.6406\nEpoch 3/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.6291 - loss: 0.7260\nEpoch 4/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8601 - loss: 0.4094\nEpoch 5/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9209 - loss: 0.2054\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step\nLSTM accuracy: 0.655\neraser_movie_20_paraphrase_10_1.obj\nEpoch 1/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5907 - loss: 0.6864\nEpoch 2/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.7747 - loss: 0.5243\nEpoch 3/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9371 - loss: 0.1912\nEpoch 4/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9637 - loss: 0.0745\nEpoch 5/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9635 - loss: 0.0746\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step\nLSTM accuracy: 0.56\neraser_movie_20_paraphrase_10_2.obj\nEpoch 1/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5700 - loss: 0.6817\nEpoch 2/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8730 - loss: 0.3879\nEpoch 3/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9356 - loss: 0.2858\nEpoch 4/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9777 - loss: 0.0721\nEpoch 5/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9821 - loss: 0.0550\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step\nLSTM accuracy: 0.595\neraser_movie_20_paraphrase_10_4.obj\nEpoch 1/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5687 - loss: 0.6813\nEpoch 2/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.8656 - loss: 0.3608\nEpoch 3/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9640 - loss: 0.1152\nEpoch 4/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9692 - loss: 0.0933\nEpoch 5/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9676 - loss: 0.0897\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step\nLSTM accuracy: 0.595\neraser_movie_20_paraphrase_10_3.obj\nEpoch 1/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5877 - loss: 0.6854\nEpoch 2/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8347 - loss: 0.4570\nEpoch 3/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9370 - loss: 0.2090\nEpoch 4/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9568 - loss: 0.0972\nEpoch 5/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.9641 - loss: 0.0742\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step\nLSTM accuracy: 0.6\neraser_movie_10_paraphrase_10_3.obj\nEpoch 1/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5740 - loss: 0.6804\nEpoch 2/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9054 - loss: 0.3624\nEpoch 3/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9581 - loss: 0.1459\nEpoch 4/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9463 - loss: 0.0974\nEpoch 5/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9547 - loss: 0.0721\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step\nLSTM accuracy: 0.485\neraser_movie_10_paraphrase_10_2.obj\nEpoch 1/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6375 - loss: 0.6691\nEpoch 2/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.8834 - loss: 0.4565\nEpoch 3/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9488 - loss: 0.2247\nEpoch 4/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9531 - loss: 0.0986\nEpoch 5/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9569 - loss: 0.0840\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step\nLSTM accuracy: 0.5\neraser_movie_50_paraphrase_10_4.obj\nEpoch 1/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5588 - loss: 0.6909\nEpoch 2/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.7241 - loss: 0.6508\nEpoch 3/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8116 - loss: 0.5199\nEpoch 4/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9228 - loss: 0.2000\nEpoch 5/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9455 - loss: 0.1011\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step\nLSTM accuracy: 0.515\neraser_movie_10_paraphrase_10_0.obj\nEpoch 1/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6571 - loss: 0.6730\nEpoch 2/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.8499 - loss: 0.4138\nEpoch 3/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9552 - loss: 0.1666\nEpoch 4/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9617 - loss: 0.0701\nEpoch 5/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9478 - loss: 0.1260\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step\nLSTM accuracy: 0.525\neraser_movie_50_paraphrase_10_0.obj\nEpoch 1/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5208 - loss: 0.6905\nEpoch 2/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.6969 - loss: 0.6441\nEpoch 3/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8892 - loss: 0.3290\nEpoch 4/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9462 - loss: 0.1224\nEpoch 5/5\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9703 - loss: 0.0719\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step\nLSTM accuracy: 0.585\neraser_movie_20_paraphrase_10_0.obj\nSVM accuracy: 0.545\neraser_movie_10_paraphrase_10_1.obj\nSVM accuracy: 0.5\neraser_movie_50_paraphrase_10_3.obj\nSVM accuracy: 0.78\neraser_movie_50_paraphrase_10_2.obj\nSVM accuracy: 0.775\neraser_movie_10_paraphrase_10_4.obj\nSVM accuracy: 0.51\neraser_movie_50_paraphrase_10_1.obj\nSVM accuracy: 0.82\neraser_movie_20_paraphrase_10_1.obj\nSVM accuracy: 0.525\neraser_movie_20_paraphrase_10_2.obj\nSVM accuracy: 0.535\neraser_movie_20_paraphrase_10_4.obj\nSVM accuracy: 0.52\neraser_movie_20_paraphrase_10_3.obj\nSVM accuracy: 0.53\neraser_movie_10_paraphrase_10_3.obj\nSVM accuracy: 0.505\neraser_movie_10_paraphrase_10_2.obj\nSVM accuracy: 0.505\neraser_movie_50_paraphrase_10_4.obj\nSVM accuracy: 0.81\neraser_movie_10_paraphrase_10_0.obj\nSVM accuracy: 0.505\neraser_movie_50_paraphrase_10_0.obj\nSVM accuracy: 0.78\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\n# Number of trained models\nlen(list(models.items()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:39:58.018056Z","iopub.execute_input":"2025-01-11T13:39:58.018340Z","iopub.status.idle":"2025-01-11T13:39:58.024543Z","shell.execute_reply.started":"2025-01-11T13:39:58.018313Z","shell.execute_reply":"2025-01-11T13:39:58.023655Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"30"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Method used to decode train dataset names and extracting balancing method and imbalance percentage\ndef get_method_and_percentage(name):\n    string = \"_\".join(name.split(\"_\")[:-1])\n    percentage = \"\"\n    perc = 0\n    for i in range(len(string)):\n        if string[i].isdigit():\n            perc = i\n            percentage += string[i]\n    perc+=2 if len(percentage)>0 else 0\n    method = string[perc:]\n    return [method, percentage]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:39:58.025531Z","iopub.execute_input":"2025-01-11T13:39:58.025781Z","iopub.status.idle":"2025-01-11T13:39:58.036645Z","shell.execute_reply.started":"2025-01-11T13:39:58.025756Z","shell.execute_reply":"2025-01-11T13:39:58.035932Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# GGathering the results and converting them to .cssv file\ndf = pd.DataFrame(list(models.items()))\nres = pd.DataFrame(list(df[1]))\nname = pd.DataFrame(list(df[0].map(lambda x:get_method_and_percentage(x)+[x.split(\"*\")[1], x.split(\"_\")[-1].split(\"*\")[0]])))\nname.columns = [\"Balancing method\", \"Imbalance %\", \"Model name\", \"Model ID\"]\ndf = pd.concat([name, res], axis=1)\ndf = df.sort_values([\"Imbalance %\", \"Model ID\", \"Balancing method\", \"Model name\"])\ndf.columns = [str(col).replace(\"eval_\", \"\") for col in df.columns]\ndf.to_csv(\"nlpaug_paraphrasers_distilbert.csv\")\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:39:58.037821Z","iopub.execute_input":"2025-01-11T13:39:58.038058Z","iopub.status.idle":"2025-01-11T13:39:58.079708Z","shell.execute_reply.started":"2025-01-11T13:39:58.038035Z","shell.execute_reply":"2025-01-11T13:39:58.078821Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"   Balancing method Imbalance % Model name Model ID    acc        f1  \\\n13                         1010       LSTM        0  0.525  0.227642   \n28                         1010        SVM        0  0.505  0.019802   \n1                          1010       LSTM        1  0.500  0.090909   \n16                         1010        SVM        1  0.500  0.019608   \n11                         1010       LSTM        2  0.500  0.000000   \n26                         1010        SVM        2  0.505  0.019802   \n10                         1010       LSTM        3  0.485  0.019048   \n25                         1010        SVM        3  0.505  0.019802   \n4                          1010       LSTM        4  0.500  0.000000   \n19                         1010        SVM        4  0.510  0.039216   \n0                          2010       LSTM        0  0.550  0.274194   \n15                         2010        SVM        0  0.545  0.165138   \n6                          2010       LSTM        1  0.560  0.228070   \n21                         2010        SVM        1  0.525  0.095238   \n7                          2010       LSTM        2  0.595  0.381679   \n22                         2010        SVM        2  0.535  0.130841   \n9                          2010       LSTM        3  0.600  0.384615   \n24                         2010        SVM        3  0.530  0.113208   \n8                          2010       LSTM        4  0.595  0.470588   \n23                         2010        SVM        4  0.520  0.076923   \n14                         5010       LSTM        0  0.585  0.464516   \n29                         5010        SVM        0  0.780  0.738095   \n5                          5010       LSTM        1  0.655  0.586826   \n20                         5010        SVM        1  0.820  0.790698   \n3                          5010       LSTM        2  0.555  0.288000   \n18                         5010        SVM        2  0.775  0.716981   \n2                          5010       LSTM        3  0.645  0.603352   \n17                         5010        SVM        3  0.780  0.738095   \n12                         5010       LSTM        4  0.515  0.374194   \n27                         5010        SVM        4  0.810  0.776471   \n\n    precision  recall       mcc      auc  spearman  spearman_pval  \n13   0.608696    0.14  0.078365  0.60360  0.602671   6.516811e-02  \n28   1.000000    0.01  0.070888  0.72400  0.903030   3.436122e-04  \n1    0.500000    0.05  0.000000  0.68100  0.873950   9.460263e-04  \n16   0.500000    0.01  0.000000  0.73240  0.843177   2.180016e-03  \n11   0.000000    0.00  0.000000  0.62010  0.725623   1.752806e-02  \n26   1.000000    0.01  0.070888  0.64830  0.789024   6.660472e-03  \n10   0.200000    0.01 -0.096077  0.56230  0.594567   6.985135e-02  \n25   1.000000    0.01  0.070888  0.67070  0.781818   7.547008e-03  \n4    0.000000    0.00  0.000000  0.61740  0.640043   4.623588e-02  \n19   1.000000    0.02  0.100504  0.70570  0.887542   6.096660e-04  \n0    0.708333    0.17  0.153864  0.65050  0.800391   5.416207e-03  \n15   1.000000    0.09  0.217072  0.85840  0.960491   1.016324e-05  \n6    0.928571    0.13  0.235159  0.68540  0.814593   4.106132e-03  \n21   1.000000    0.05  0.160128  0.79760  0.923586   1.359168e-04  \n7    0.806452    0.25  0.262500  0.69610  0.914651   2.092088e-04  \n22   1.000000    0.07  0.190445  0.81690  0.945140   3.707957e-05  \n9    0.833333    0.25  0.280056  0.67720  0.808781   4.611133e-03  \n24   1.000000    0.06  0.175863  0.83240  0.966268   5.438097e-06  \n8    0.679245    0.36  0.215257  0.67370  0.928037   1.075005e-04  \n23   1.000000    0.04  0.142857  0.78000  0.896358   4.446887e-04  \n14   0.654545    0.36  0.190363  0.62350  0.769322   9.277629e-03  \n29   0.911765    0.62  0.591080  0.90460  0.972649   2.368935e-06  \n5    0.731343    0.49  0.328396  0.73150  0.889638   5.669647e-04  \n20   0.944444    0.68  0.666667  0.91660  0.966402   5.353498e-06  \n3    0.720000    0.18  0.166304  0.70710  0.911353   2.424732e-04  \n18   0.966102    0.57  0.603014  0.90845  1.000000   6.646897e-64  \n2    0.683544    0.54  0.296614  0.67470  0.798795   5.579938e-03  \n17   0.911765    0.62  0.591080  0.90700  0.972649   2.368935e-06  \n12   0.527273    0.29  0.033594  0.55900  0.538503   1.082890e-01  \n27   0.942857    0.66  0.649937  0.91260  0.972649   2.368935e-06  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Balancing method</th>\n      <th>Imbalance %</th>\n      <th>Model name</th>\n      <th>Model ID</th>\n      <th>acc</th>\n      <th>f1</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>mcc</th>\n      <th>auc</th>\n      <th>spearman</th>\n      <th>spearman_pval</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13</th>\n      <td></td>\n      <td>1010</td>\n      <td>LSTM</td>\n      <td>0</td>\n      <td>0.525</td>\n      <td>0.227642</td>\n      <td>0.608696</td>\n      <td>0.14</td>\n      <td>0.078365</td>\n      <td>0.60360</td>\n      <td>0.602671</td>\n      <td>6.516811e-02</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td></td>\n      <td>1010</td>\n      <td>SVM</td>\n      <td>0</td>\n      <td>0.505</td>\n      <td>0.019802</td>\n      <td>1.000000</td>\n      <td>0.01</td>\n      <td>0.070888</td>\n      <td>0.72400</td>\n      <td>0.903030</td>\n      <td>3.436122e-04</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td></td>\n      <td>1010</td>\n      <td>LSTM</td>\n      <td>1</td>\n      <td>0.500</td>\n      <td>0.090909</td>\n      <td>0.500000</td>\n      <td>0.05</td>\n      <td>0.000000</td>\n      <td>0.68100</td>\n      <td>0.873950</td>\n      <td>9.460263e-04</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td></td>\n      <td>1010</td>\n      <td>SVM</td>\n      <td>1</td>\n      <td>0.500</td>\n      <td>0.019608</td>\n      <td>0.500000</td>\n      <td>0.01</td>\n      <td>0.000000</td>\n      <td>0.73240</td>\n      <td>0.843177</td>\n      <td>2.180016e-03</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td></td>\n      <td>1010</td>\n      <td>LSTM</td>\n      <td>2</td>\n      <td>0.500</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.62010</td>\n      <td>0.725623</td>\n      <td>1.752806e-02</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td></td>\n      <td>1010</td>\n      <td>SVM</td>\n      <td>2</td>\n      <td>0.505</td>\n      <td>0.019802</td>\n      <td>1.000000</td>\n      <td>0.01</td>\n      <td>0.070888</td>\n      <td>0.64830</td>\n      <td>0.789024</td>\n      <td>6.660472e-03</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td></td>\n      <td>1010</td>\n      <td>LSTM</td>\n      <td>3</td>\n      <td>0.485</td>\n      <td>0.019048</td>\n      <td>0.200000</td>\n      <td>0.01</td>\n      <td>-0.096077</td>\n      <td>0.56230</td>\n      <td>0.594567</td>\n      <td>6.985135e-02</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td></td>\n      <td>1010</td>\n      <td>SVM</td>\n      <td>3</td>\n      <td>0.505</td>\n      <td>0.019802</td>\n      <td>1.000000</td>\n      <td>0.01</td>\n      <td>0.070888</td>\n      <td>0.67070</td>\n      <td>0.781818</td>\n      <td>7.547008e-03</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td></td>\n      <td>1010</td>\n      <td>LSTM</td>\n      <td>4</td>\n      <td>0.500</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.61740</td>\n      <td>0.640043</td>\n      <td>4.623588e-02</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td></td>\n      <td>1010</td>\n      <td>SVM</td>\n      <td>4</td>\n      <td>0.510</td>\n      <td>0.039216</td>\n      <td>1.000000</td>\n      <td>0.02</td>\n      <td>0.100504</td>\n      <td>0.70570</td>\n      <td>0.887542</td>\n      <td>6.096660e-04</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td></td>\n      <td>2010</td>\n      <td>LSTM</td>\n      <td>0</td>\n      <td>0.550</td>\n      <td>0.274194</td>\n      <td>0.708333</td>\n      <td>0.17</td>\n      <td>0.153864</td>\n      <td>0.65050</td>\n      <td>0.800391</td>\n      <td>5.416207e-03</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td></td>\n      <td>2010</td>\n      <td>SVM</td>\n      <td>0</td>\n      <td>0.545</td>\n      <td>0.165138</td>\n      <td>1.000000</td>\n      <td>0.09</td>\n      <td>0.217072</td>\n      <td>0.85840</td>\n      <td>0.960491</td>\n      <td>1.016324e-05</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td></td>\n      <td>2010</td>\n      <td>LSTM</td>\n      <td>1</td>\n      <td>0.560</td>\n      <td>0.228070</td>\n      <td>0.928571</td>\n      <td>0.13</td>\n      <td>0.235159</td>\n      <td>0.68540</td>\n      <td>0.814593</td>\n      <td>4.106132e-03</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td></td>\n      <td>2010</td>\n      <td>SVM</td>\n      <td>1</td>\n      <td>0.525</td>\n      <td>0.095238</td>\n      <td>1.000000</td>\n      <td>0.05</td>\n      <td>0.160128</td>\n      <td>0.79760</td>\n      <td>0.923586</td>\n      <td>1.359168e-04</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td></td>\n      <td>2010</td>\n      <td>LSTM</td>\n      <td>2</td>\n      <td>0.595</td>\n      <td>0.381679</td>\n      <td>0.806452</td>\n      <td>0.25</td>\n      <td>0.262500</td>\n      <td>0.69610</td>\n      <td>0.914651</td>\n      <td>2.092088e-04</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td></td>\n      <td>2010</td>\n      <td>SVM</td>\n      <td>2</td>\n      <td>0.535</td>\n      <td>0.130841</td>\n      <td>1.000000</td>\n      <td>0.07</td>\n      <td>0.190445</td>\n      <td>0.81690</td>\n      <td>0.945140</td>\n      <td>3.707957e-05</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td></td>\n      <td>2010</td>\n      <td>LSTM</td>\n      <td>3</td>\n      <td>0.600</td>\n      <td>0.384615</td>\n      <td>0.833333</td>\n      <td>0.25</td>\n      <td>0.280056</td>\n      <td>0.67720</td>\n      <td>0.808781</td>\n      <td>4.611133e-03</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td></td>\n      <td>2010</td>\n      <td>SVM</td>\n      <td>3</td>\n      <td>0.530</td>\n      <td>0.113208</td>\n      <td>1.000000</td>\n      <td>0.06</td>\n      <td>0.175863</td>\n      <td>0.83240</td>\n      <td>0.966268</td>\n      <td>5.438097e-06</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td></td>\n      <td>2010</td>\n      <td>LSTM</td>\n      <td>4</td>\n      <td>0.595</td>\n      <td>0.470588</td>\n      <td>0.679245</td>\n      <td>0.36</td>\n      <td>0.215257</td>\n      <td>0.67370</td>\n      <td>0.928037</td>\n      <td>1.075005e-04</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td></td>\n      <td>2010</td>\n      <td>SVM</td>\n      <td>4</td>\n      <td>0.520</td>\n      <td>0.076923</td>\n      <td>1.000000</td>\n      <td>0.04</td>\n      <td>0.142857</td>\n      <td>0.78000</td>\n      <td>0.896358</td>\n      <td>4.446887e-04</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td></td>\n      <td>5010</td>\n      <td>LSTM</td>\n      <td>0</td>\n      <td>0.585</td>\n      <td>0.464516</td>\n      <td>0.654545</td>\n      <td>0.36</td>\n      <td>0.190363</td>\n      <td>0.62350</td>\n      <td>0.769322</td>\n      <td>9.277629e-03</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td></td>\n      <td>5010</td>\n      <td>SVM</td>\n      <td>0</td>\n      <td>0.780</td>\n      <td>0.738095</td>\n      <td>0.911765</td>\n      <td>0.62</td>\n      <td>0.591080</td>\n      <td>0.90460</td>\n      <td>0.972649</td>\n      <td>2.368935e-06</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td></td>\n      <td>5010</td>\n      <td>LSTM</td>\n      <td>1</td>\n      <td>0.655</td>\n      <td>0.586826</td>\n      <td>0.731343</td>\n      <td>0.49</td>\n      <td>0.328396</td>\n      <td>0.73150</td>\n      <td>0.889638</td>\n      <td>5.669647e-04</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td></td>\n      <td>5010</td>\n      <td>SVM</td>\n      <td>1</td>\n      <td>0.820</td>\n      <td>0.790698</td>\n      <td>0.944444</td>\n      <td>0.68</td>\n      <td>0.666667</td>\n      <td>0.91660</td>\n      <td>0.966402</td>\n      <td>5.353498e-06</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td></td>\n      <td>5010</td>\n      <td>LSTM</td>\n      <td>2</td>\n      <td>0.555</td>\n      <td>0.288000</td>\n      <td>0.720000</td>\n      <td>0.18</td>\n      <td>0.166304</td>\n      <td>0.70710</td>\n      <td>0.911353</td>\n      <td>2.424732e-04</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td></td>\n      <td>5010</td>\n      <td>SVM</td>\n      <td>2</td>\n      <td>0.775</td>\n      <td>0.716981</td>\n      <td>0.966102</td>\n      <td>0.57</td>\n      <td>0.603014</td>\n      <td>0.90845</td>\n      <td>1.000000</td>\n      <td>6.646897e-64</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td></td>\n      <td>5010</td>\n      <td>LSTM</td>\n      <td>3</td>\n      <td>0.645</td>\n      <td>0.603352</td>\n      <td>0.683544</td>\n      <td>0.54</td>\n      <td>0.296614</td>\n      <td>0.67470</td>\n      <td>0.798795</td>\n      <td>5.579938e-03</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td></td>\n      <td>5010</td>\n      <td>SVM</td>\n      <td>3</td>\n      <td>0.780</td>\n      <td>0.738095</td>\n      <td>0.911765</td>\n      <td>0.62</td>\n      <td>0.591080</td>\n      <td>0.90700</td>\n      <td>0.972649</td>\n      <td>2.368935e-06</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td></td>\n      <td>5010</td>\n      <td>LSTM</td>\n      <td>4</td>\n      <td>0.515</td>\n      <td>0.374194</td>\n      <td>0.527273</td>\n      <td>0.29</td>\n      <td>0.033594</td>\n      <td>0.55900</td>\n      <td>0.538503</td>\n      <td>1.082890e-01</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td></td>\n      <td>5010</td>\n      <td>SVM</td>\n      <td>4</td>\n      <td>0.810</td>\n      <td>0.776471</td>\n      <td>0.942857</td>\n      <td>0.66</td>\n      <td>0.649937</td>\n      <td>0.91260</td>\n      <td>0.972649</td>\n      <td>2.368935e-06</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13}]}