{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:54:43.789664Z",
     "iopub.status.busy": "2024-11-27T20:54:43.789221Z",
     "iopub.status.idle": "2024-11-27T20:55:06.673066Z",
     "shell.execute_reply": "2024-11-27T20:55:06.672045Z",
     "shell.execute_reply.started": "2024-11-27T20:54:43.789622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "\n",
    "from shutil import copyfile\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (InputFeatures)\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Dict\n",
    "from transformers import InputFeatures, T5ForConditionalGeneration, T5TokenizerFast\n",
    "\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:06.676231Z",
     "iopub.status.busy": "2024-11-27T20:55:06.675329Z",
     "iopub.status.idle": "2024-11-27T20:55:12.343038Z",
     "shell.execute_reply": "2024-11-27T20:55:12.342037Z",
     "shell.execute_reply.started": "2024-11-27T20:55:06.676164Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# copyfile(src = \"C:/Users/Maciek/Documents/Studia/Magisterka/GitHub/BalancingMethodsNLP/Proof of Concept/utils.py\", dst = \"/working/utils.py\")\n",
    "# copyfile(src = \"/kaggle/input/balancing-script/Class Balancing.py\", dst = \"../working/Class_Balancing.py\")\n",
    "from utils import load_documents, load_datasets, annotations_from_jsonl, Annotation\n",
    "from Class_Balancing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:12.344697Z",
     "iopub.status.busy": "2024-11-27T20:55:12.344300Z",
     "iopub.status.idle": "2024-11-27T20:55:29.521309Z",
     "shell.execute_reply": "2024-11-27T20:55:29.520178Z",
     "shell.execute_reply.started": "2024-11-27T20:55:12.344660Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# data_root = r'C:\\Users\\Maciek\\Documents\\Studia\\Magisterka\\eraser'\n",
    "# documents = load_documents(data_root)\n",
    "\n",
    "# train, val, test = load_datasets(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:29.523975Z",
     "iopub.status.busy": "2024-11-27T20:55:29.523591Z",
     "iopub.status.idle": "2024-11-27T20:55:29.530212Z",
     "shell.execute_reply": "2024-11-27T20:55:29.528933Z",
     "shell.execute_reply.started": "2024-11-27T20:55:29.523909Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def get_input(ann):\n",
    "#     if len(ann.all_evidences())==0:\n",
    "#         docid  = ann.annotation_id\n",
    "#     else:\n",
    "#         (docid,) = set(ev.docid for ev in ann.all_evidences())\n",
    "#     doc = documents[docid]\n",
    "#     input = ''\n",
    "#     for sent in doc:\n",
    "#         input += ' '.join(sent)\n",
    "#     return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:29.531937Z",
     "iopub.status.busy": "2024-11-27T20:55:29.531567Z",
     "iopub.status.idle": "2024-11-27T20:55:29.557295Z",
     "shell.execute_reply": "2024-11-27T20:55:29.556090Z",
     "shell.execute_reply.started": "2024-11-27T20:55:29.531899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def print_color(text, evidences):\n",
    "#     for ev in evidences:\n",
    "#         t = \"\\033[95m\" + ev+ '\\x1b[0m'\n",
    "#         if ev!='':\n",
    "#             text = t.join(text.split(ev))\n",
    "#     print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:29.559025Z",
     "iopub.status.busy": "2024-11-27T20:55:29.558578Z",
     "iopub.status.idle": "2024-11-27T20:55:29.574272Z",
     "shell.execute_reply": "2024-11-27T20:55:29.573108Z",
     "shell.execute_reply.started": "2024-11-27T20:55:29.558976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TrainerDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, tokenizer, evidences=None):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.evidences = evidences\n",
    "\n",
    "        # Tokenize the input\n",
    "        self.tokenized_inputs = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return InputFeatures(\n",
    "            input_ids=self.tokenized_inputs['input_ids'][idx],\n",
    "#             token_type_ids=self.tokenized_inputs['token_type_ids'][idx],\n",
    "            attention_mask=self.tokenized_inputs['attention_mask'][idx],\n",
    "            label=self.targets[idx])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:29.576137Z",
     "iopub.status.busy": "2024-11-27T20:55:29.575752Z",
     "iopub.status.idle": "2024-11-27T20:55:29.592156Z",
     "shell.execute_reply": "2024-11-27T20:55:29.591015Z",
     "shell.execute_reply.started": "2024-11-27T20:55:29.576104Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def create_dataset(dataset, tokenizer):\n",
    "#     targets = [1  if ann.classification != 'NEG' else 0 for ann in dataset]\n",
    "#     evidences = [[ev.text for ev in ann.all_evidences()] for ann in dataset]\n",
    "#     inputs = [get_input(ann) for ann in dataset]\n",
    "    \n",
    "#     if len(targets)==len(evidences)==len(inputs):\n",
    "#         return TrainerDataset(inputs, targets, tokenizer, evidences)\n",
    "#     else:\n",
    "#         print(\"Something went wrong !!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:29.594262Z",
     "iopub.status.busy": "2024-11-27T20:55:29.593784Z",
     "iopub.status.idle": "2024-11-27T20:55:30.756461Z",
     "shell.execute_reply": "2024-11-27T20:55:30.755204Z",
     "shell.execute_reply.started": "2024-11-27T20:55:29.594218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:30.758651Z",
     "iopub.status.busy": "2024-11-27T20:55:30.758196Z",
     "iopub.status.idle": "2024-11-27T20:55:34.206813Z",
     "shell.execute_reply": "2024-11-27T20:55:34.205930Z",
     "shell.execute_reply.started": "2024-11-27T20:55:30.758601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_dataset = create_dataset(train, tokenizer)\n",
    "# eval_dataset = create_dataset(val, tokenizer)\n",
    "# test_dataset = create_dataset(test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:34.211819Z",
     "iopub.status.busy": "2024-11-27T20:55:34.211400Z",
     "iopub.status.idle": "2024-11-27T20:55:35.819728Z",
     "shell.execute_reply": "2024-11-27T20:55:35.818202Z",
     "shell.execute_reply.started": "2024-11-27T20:55:34.211789Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:35.822353Z",
     "iopub.status.busy": "2024-11-27T20:55:35.821831Z",
     "iopub.status.idle": "2024-11-27T20:55:36.911773Z",
     "shell.execute_reply": "2024-11-27T20:55:36.910817Z",
     "shell.execute_reply.started": "2024-11-27T20:55:35.822299Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "paraphraser_t5small = T5ForConditionalGeneration.from_pretrained(\"mrm8488/t5-small-finetuned-quora-for-paraphrasing\")\n",
    "tokenizer_t5small = T5TokenizerFast.from_pretrained(\"mrm8488/t5-small-finetuned-quora-for-paraphrasing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:36.913604Z",
     "iopub.status.busy": "2024-11-27T20:55:36.913308Z",
     "iopub.status.idle": "2024-11-27T20:55:37.323410Z",
     "shell.execute_reply": "2024-11-27T20:55:37.322517Z",
     "shell.execute_reply.started": "2024-11-27T20:55:36.913576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# dataset = \"eraser_movie\"\n",
    "\n",
    "# filehandler = open(f'{dataset}_train.obj',\"wb\")\n",
    "# pickle.dump(train_dataset,filehandler)\n",
    "# filehandler.close()\n",
    "\n",
    "# filehandler = open(f'{dataset}_eval.obj',\"wb\")\n",
    "# pickle.dump(eval_dataset,filehandler)\n",
    "# filehandler.close()\n",
    "\n",
    "# filehandler = open(f'{dataset}_test.obj',\"wb\")\n",
    "# pickle.dump(test_dataset,filehandler)\n",
    "# filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:37.324958Z",
     "iopub.status.busy": "2024-11-27T20:55:37.324608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# imbalance = 0.1\n",
    "# random_seed = 123\n",
    "# np.random.seed(123)\n",
    "# for syn_prob in [0.01, 0.02, 0.05, 0.1, 0.2]:\n",
    "#     for i in range(20):\n",
    "#         train_dataset_imbalanced = add_imbalance(train_dataset, imbalance, random_seed = np.random.randint(0,123))\n",
    "#         train_dataset_synonym = balance_minority(train_dataset_imbalanced, replace_synonym, **{\"prob\":syn_prob}, random_seed = np.random.randint(0,123))\n",
    "\n",
    "#         filehandler = open(fr'C:\\Users\\Maciek\\Documents\\Studia\\Magisterka\\Synonym_probability\\{dataset}_{int(imbalance*100)}_synonym_{int(syn_prob*100)}%_{i}.obj',\"wb\")\n",
    "#         pickle.dump(train_dataset_synonym,filehandler)\n",
    "#         filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "218\n",
      "3430\n",
      "4170\n",
      "5886\n",
      "1617\n",
      "2280\n",
      "3923\n",
      "4559\n",
      "5213\n",
      "1866\n",
      "2302\n",
      "3683\n",
      "4083\n",
      "5490\n",
      "8887\r"
     ]
    }
   ],
   "source": [
    "imbalance = 0.5\n",
    "par_perc = 0.1\n",
    "task = \"eraser_movie\"\n",
    "np.random.seed(123)\n",
    "for imbalance in [10, 20, 50]:\n",
    "    for i in range(5):\n",
    "        print(i+1)\n",
    "        with open(fr\"C:\\Users\\Maciek\\Documents\\Studia\\Magisterka\\GitHub\\BalancingMethodsNLP\\nlpaug\\{task}_{imbalance}_imbalanced_{i}.obj\", 'rb') as pickle_file:\n",
    "            train_dataset_imbalanced = pickle.load(pickle_file)\n",
    "        train_dataset_paraphrase = balance_minority(train_dataset_imbalanced, paraphrase_perc_sentences, False, random_seed = np.random.randint(0,123), **{\"p\":par_perc, \"paraphraser\":paraphraser_t5small, \"tokenizer\":tokenizer_t5small, \"count\":True})\n",
    "        \n",
    "        \n",
    "        filehandler = open(fr'C:\\Users\\Maciek\\Documents\\Studia\\Magisterka\\GitHub\\BalancingMethodsNLP\\paraphrased\\{task}_{imbalance}_paraphrase_{int(par_perc*100)}%_{i}.obj',\"wb\")\n",
    "        pickle.dump(train_dataset_paraphrase,filehandler)\n",
    "        filehandler.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6061116,
     "sourceId": 9873183,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6155701,
     "sourceId": 10006502,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
