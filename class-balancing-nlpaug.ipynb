{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:54:43.789664Z",
     "iopub.status.busy": "2024-11-27T20:54:43.789221Z",
     "iopub.status.idle": "2024-11-27T20:55:06.673066Z",
     "shell.execute_reply": "2024-11-27T20:55:06.672045Z",
     "shell.execute_reply.started": "2024-11-27T20:54:43.789622Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d81bd5c50842a2ba677b22c7e2d9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6852d1cb49745cbbbe9245a7c016df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/419M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4a387a8b4a45fe998a95ae918360e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241da7244fd942cbb3aca56d05ec19ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ff9b9d168d4909bbe62d2998293183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, InputFeatures\n",
    "\n",
    "import time\n",
    "\n",
    "from utils import load_documents, load_datasets\n",
    "from Class_Balancing import *\n",
    "\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:12.344697Z",
     "iopub.status.busy": "2024-11-27T20:55:12.344300Z",
     "iopub.status.idle": "2024-11-27T20:55:29.521309Z",
     "shell.execute_reply": "2024-11-27T20:55:29.520178Z",
     "shell.execute_reply.started": "2024-11-27T20:55:12.344660Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_root = r'C:\\Users\\Maciek\\Documents\\Studia\\Magisterka\\eraser'\n",
    "documents = load_documents(data_root)\n",
    "\n",
    "train, val, test = load_datasets(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:29.523975Z",
     "iopub.status.busy": "2024-11-27T20:55:29.523591Z",
     "iopub.status.idle": "2024-11-27T20:55:29.530212Z",
     "shell.execute_reply": "2024-11-27T20:55:29.528933Z",
     "shell.execute_reply.started": "2024-11-27T20:55:29.523909Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_input(ann):\n",
    "    if len(ann.all_evidences())==0:\n",
    "        docid  = ann.annotation_id\n",
    "    else:\n",
    "        (docid,) = set(ev.docid for ev in ann.all_evidences())\n",
    "    doc = documents[docid]\n",
    "    input = ''\n",
    "    for sent in doc:\n",
    "        input += ' '.join(sent)\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:29.531937Z",
     "iopub.status.busy": "2024-11-27T20:55:29.531567Z",
     "iopub.status.idle": "2024-11-27T20:55:29.557295Z",
     "shell.execute_reply": "2024-11-27T20:55:29.556090Z",
     "shell.execute_reply.started": "2024-11-27T20:55:29.531899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_color(text, evidences):\n",
    "    for ev in evidences:\n",
    "        t = \"\\033[95m\" + ev+ '\\x1b[0m'\n",
    "        if ev!='':\n",
    "            text = t.join(text.split(ev))\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:29.559025Z",
     "iopub.status.busy": "2024-11-27T20:55:29.558578Z",
     "iopub.status.idle": "2024-11-27T20:55:29.574272Z",
     "shell.execute_reply": "2024-11-27T20:55:29.573108Z",
     "shell.execute_reply.started": "2024-11-27T20:55:29.558976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TrainerDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, tokenizer, evidences=None):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.evidences = evidences\n",
    "\n",
    "        # Tokenize the input\n",
    "        self.tokenized_inputs = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return InputFeatures(\n",
    "            input_ids=self.tokenized_inputs['input_ids'][idx],\n",
    "            attention_mask=self.tokenized_inputs['attention_mask'][idx],\n",
    "            label=self.targets[idx])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:29.576137Z",
     "iopub.status.busy": "2024-11-27T20:55:29.575752Z",
     "iopub.status.idle": "2024-11-27T20:55:29.592156Z",
     "shell.execute_reply": "2024-11-27T20:55:29.591015Z",
     "shell.execute_reply.started": "2024-11-27T20:55:29.576104Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(dataset, tokenizer):\n",
    "    targets = [1  if ann.classification != 'NEG' else 0 for ann in dataset]\n",
    "    evidences = [[ev.text for ev in ann.all_evidences()] for ann in dataset]\n",
    "    inputs = [get_input(ann) for ann in dataset]\n",
    "    \n",
    "    if len(targets)==len(evidences)==len(inputs):\n",
    "        return TrainerDataset(inputs, targets, tokenizer, evidences)\n",
    "    else:\n",
    "        print(\"Something went wrong !!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:29.594262Z",
     "iopub.status.busy": "2024-11-27T20:55:29.593784Z",
     "iopub.status.idle": "2024-11-27T20:55:30.756461Z",
     "shell.execute_reply": "2024-11-27T20:55:30.755204Z",
     "shell.execute_reply.started": "2024-11-27T20:55:29.594218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:30.758651Z",
     "iopub.status.busy": "2024-11-27T20:55:30.758196Z",
     "iopub.status.idle": "2024-11-27T20:55:34.206813Z",
     "shell.execute_reply": "2024-11-27T20:55:34.205930Z",
     "shell.execute_reply.started": "2024-11-27T20:55:30.758601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(train, tokenizer)\n",
    "eval_dataset = create_dataset(val, tokenizer)\n",
    "test_dataset = create_dataset(test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:55:36.913604Z",
     "iopub.status.busy": "2024-11-27T20:55:36.913308Z",
     "iopub.status.idle": "2024-11-27T20:55:37.323410Z",
     "shell.execute_reply": "2024-11-27T20:55:37.322517Z",
     "shell.execute_reply.started": "2024-11-27T20:55:36.913576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "dataset = \"eraser_movie\"\n",
    "\n",
    "filehandler = open(f'{dataset}_train.obj',\"wb\")\n",
    "pickle.dump(train_dataset,filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open(f'{dataset}_eval.obj',\"wb\")\n",
    "pickle.dump(eval_dataset,filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open(f'{dataset}_test.obj',\"wb\")\n",
    "pickle.dump(test_dataset,filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(time):\n",
    "    \"\"\" Method converts number of seconds into time in format ___ h __ m __.__ s\n",
    "    Args:\n",
    "        time (float): Number of seconds\n",
    "    Returns:\n",
    "        str: Time in format ___ h __ m __.__ s\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    if time//3600 > 0:\n",
    "        result += str(int(time//3600)) + \" h  \"\n",
    "        time %= 3600\n",
    "    if time//60 > 0:\n",
    "        result += str(int(time//60)) + \" m  \"\n",
    "        time %= 60\n",
    "    if time//1 > 0:\n",
    "        result += str(np.round(time,2)) + \" s                      \"\n",
    "    return result\n",
    "\n",
    "\n",
    "def balance_minority(train_dataset, fun, limited_range=False, random_seed=123, **kwargs):\n",
    "    \"\"\" Method used to balance minority using some function fun (ex. replace_synonym, deepcopy, ...\n",
    "    Args:\n",
    "        train_dataset (TrainerDataset): Dataset to balance\n",
    "        fun (Callable): Function used to balance minority\n",
    "        limited_range (bool, optional): In case of time consuming balancing functions, user may choose to limit number of new examples\n",
    "                                        from the size difference between the samples to minimum of 500 and 5 * number of positive examples. \n",
    "                                        Defaults to False.\n",
    "        random_seed (int, optional): Random state. Defaults to 123.\n",
    "    Returns:\n",
    "        TrainerDataset: Balanced dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    random.seed(random_seed)\n",
    "    if limited_range:\n",
    "        global counter\n",
    "        counter = 0\n",
    "        \n",
    "    positives = np.array(train_dataset.inputs)[np.array(train_dataset.targets)==1]\n",
    "    n_positive = len(positives)\n",
    "    n_negative = len(train_dataset.targets) - n_positive\n",
    "    \n",
    "    \n",
    "    generation_count = np.min([n_negative-n_positive, 5*n_positive, 10]) if limited_range else n_negative-n_positive\n",
    "    \n",
    "    new_inputs = [\n",
    "            0 for i in range(generation_count)\n",
    "        ]\n",
    "    start_time = time.time()\n",
    "    for i in range(generation_count):\n",
    "        new_inputs[i] = fun(positives[np.random.randint(n_positive)], **kwargs)\n",
    "        print(f\"{i}/{generation_count}, est. time: {get_time((time.time()-start_time)/(i+1)*(generation_count-i))}\", end=\"\\r\")\n",
    "\n",
    "    balanced_inputs = train_dataset.inputs + new_inputs\n",
    "    balanced_targets = train_dataset.targets + [1 for _ in range(generation_count)]\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(balanced_targets)\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(balanced_inputs)\n",
    "    \n",
    "    return TrainerDataset(balanced_inputs, balanced_targets, train_dataset.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_augmentation(x, aug):\n",
    "    \"\"\" Helper method used to convert nlpaug method so that it works with balance_minority method\n",
    "    Args:\n",
    "        x (str): Text to base the augmentation on\n",
    "        aug (Callable): Function from nlpaug library\n",
    "    Returns:\n",
    "        TrainerDataset: Augmented text\n",
    "    \"\"\"\n",
    "    augment = aug.augment(x)\n",
    "    if type(augment)==list:\n",
    "        augment=augment[0]\n",
    "    if augment == None:\n",
    "        print(f\"No augmentation applied to: {x}\")\n",
    "        return x\n",
    "    return augment\n",
    "\n",
    "\n",
    "\n",
    "def create_datasets(train_dataset, imbalance = 0.05, random_seed = 123, i=0):\n",
    "    \"\"\" Method used to create imbalanced dataset, and balanced dataset based on it using selection of methods from nlpaug library and random oversampling\n",
    "    Args:\n",
    "        train_dataset (TrainerDataset): Text to base the augmentation on\n",
    "        imbalance (float): Ratio of positive to negative examples in created imbalanced dataset\n",
    "        random_seed(int): Random seed used for reproducibility purposes\n",
    "        i(str): Addition to name of resulting files\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Adding the imbalance\n",
    "    train_dataset_imbalanced = add_imbalance(train_dataset, imbalance, random_seed = random_seed)\n",
    "    \n",
    "    # Saving imbalanced dataset to file\n",
    "    filehandler = open(f'nlpaug/{dataset}_{int(imbalance*100)}_imbalanced_{i}.obj',\"wb\")\n",
    "    pickle.dump(train_dataset_imbalanced,filehandler)\n",
    "    filehandler.close()\n",
    "\n",
    "    for method in [\n",
    "        (\"Spelling_mistake\", naw.SpellingAug()),\n",
    "        (\"ROS\", deepcopy),\n",
    "        (\"Synonym replacement\", naw.SynonymAug(aug_src='wordnet')),\n",
    "        (\"Contextual_word_embedding\", naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\")),\n",
    "        (\"Summarization\", nas.AbstSummAug(model_path='t5-base')),\n",
    "        (\"Translation\", naw.BackTranslationAug(from_model_name='facebook/wmt19-en-de', to_model_name='facebook/wmt19-de-en'))\n",
    "    ]:\n",
    "        # Creating datasets for model training\n",
    "        start_time = time.time()\n",
    "        if method[0]!=\"ROS\":\n",
    "            train_dataset_augmented = balance_minority(deepcopy(train_dataset_imbalanced), lambda x: process_augmentation(x, aug=method[1]),  random_seed = random_seed)\n",
    "        else:\n",
    "            train_dataset_augmented = balance_minority(deepcopy(train_dataset_imbalanced), deepcopy,  random_seed = random_seed)\n",
    "            \n",
    "        # Quality report printing\n",
    "        print(f\"{method[0]}: {get_time(time.time()-start_time)}                         \")\n",
    "        _, counts = np.unique(train_dataset_augmented.targets, return_counts=True)\n",
    "        ratio = counts[0]/np.sum(counts)\n",
    "        print(f\"Ratio: {ratio}\")\n",
    "        print(\"Some examples from positive class\")\n",
    "        print(np.sort((np.array(train_dataset_augmented.inputs)[np.array(train_dataset_augmented.targets)==1]))[-3:])\n",
    "        print(\"========================================================================================================================================================\")\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "        # Saving created dataset to file\n",
    "        filehandler = open(f'nlpaug/{dataset}_{int(imbalance*100)}_{method[0]}_{i}.obj',\"wb\")\n",
    "        pickle.dump(train_dataset_augmented,filehandler)\n",
    "        filehandler.close()\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalance = 0.5\n",
    "random_seed = 123\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "for i in range(5):\n",
    "    create_datasets(train_dataset, imbalance = imbalance, random_seed = np.random.randint(0,random_seed), i=i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6061116,
     "sourceId": 9873183,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6155701,
     "sourceId": 10006502,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
